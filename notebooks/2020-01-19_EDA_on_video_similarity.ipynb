{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-01-19\n",
    "\n",
    "Doing some more exploratory data analysis on comment data. \n",
    "\n",
    "Want to see if I can discover the similarity between two different YT videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ronald Maj\\\\Documents\\\\GitHub\\\\InsightDataProject\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ronald Maj\\Documents\\GitHub\\InsightDataProject\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drew1_df = pd.read_csv(\"data/raw/drew1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommID</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>parentId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "      <td>tomatoanus</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l7-dWGEdE94iYyfghH...</td>\n",
       "      <td>http://www.youtube.com/channel/UCm6GSA5OROHcIB...</td>\n",
       "      <td>{'value': 'UCm6GSA5OROHcIBNkXkH53zQ'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>he was just eatin some eggs outside a girls wi...</td>\n",
       "      <td>he was just eatin some eggs outside a girls wi...</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>12447</td>\n",
       "      <td>2018-08-24T18:28:29.000Z</td>\n",
       "      <td>2018-08-24T18:28:29.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93xcOkRbfQL</td>\n",
       "      <td>Nathaniel Huerta</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l78sghDmpMleaviF44...</td>\n",
       "      <td>http://www.youtube.com/channel/UC3hksSESHg3dDJ...</td>\n",
       "      <td>{'value': 'UC3hksSESHg3dDJmiK6kPaHQ'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>He was stalking her</td>\n",
       "      <td>He was stalking her</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-19T03:30:44.000Z</td>\n",
       "      <td>2020-01-19T03:30:44.000Z</td>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93sUhWOu88m</td>\n",
       "      <td>Abe Abi Haydar</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l7-p_uJ-NlFNC1Sbjm...</td>\n",
       "      <td>http://www.youtube.com/channel/UCNib--KiuE0TaG...</td>\n",
       "      <td>{'value': 'UCNib--KiuE0TaGqIpUW9_sA'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>@Pramey Sontakke stfu like stap</td>\n",
       "      <td>@Pramey Sontakke stfu like stap</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-17T03:38:35.000Z</td>\n",
       "      <td>2020-01-17T03:38:35.000Z</td>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93eZplH5ug7</td>\n",
       "      <td>Yummyboi</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l7_yW6h1RXGK0MIe6F...</td>\n",
       "      <td>http://www.youtube.com/channel/UC_nuyvsPqhiHuF...</td>\n",
       "      <td>{'value': 'UC_nuyvsPqhiHuF2X2XlELGg'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>literally everything不</td>\n",
       "      <td>literally everything不</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-11T17:54:02.000Z</td>\n",
       "      <td>2020-01-11T17:54:02.000Z</td>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93_dNCV8tK1</td>\n",
       "      <td>Jan Wick</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l7--SwayUtPbOOD1Q0...</td>\n",
       "      <td>http://www.youtube.com/channel/UCZgWKtUWiHHK5f...</td>\n",
       "      <td>{'value': 'UCZgWKtUWiHHK5fz-AK4lK6w'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>mmmmm egg</td>\n",
       "      <td>mmmmm egg</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-09T19:57:29.000Z</td>\n",
       "      <td>2020-01-09T19:57:29.000Z</td>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93JojN1C7qc</td>\n",
       "      <td>qwerty</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l79llzvO-jCC7m5WLO...</td>\n",
       "      <td>http://www.youtube.com/channel/UCC0Uq-HTbCA7uU...</td>\n",
       "      <td>{'value': 'UCC0Uq-HTbCA7uUnHsGV6V8A'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>ITS ON A THURSDAY NIGHT</td>\n",
       "      <td>ITS ON A THURSDAY NIGHT</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03T07:09:41.000Z</td>\n",
       "      <td>2020-01-03T07:09:41.000Z</td>\n",
       "      <td>UgyMAptldbaotq25pgx4AaABAg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ugwga9AWIlVnajO3RR94AaABAg</td>\n",
       "      <td>Wendy McCoy</td>\n",
       "      <td>https://yt3.ggpht.com/a/AGF-l7_lxlfY6eiEHLrWNg...</td>\n",
       "      <td>http://www.youtube.com/channel/UCsPwH-Fb4ffPK2...</td>\n",
       "      <td>{'value': 'UCsPwH-Fb4ffPK2Upamn65wQ'}</td>\n",
       "      <td>rSLWFA_glbk</td>\n",
       "      <td>Apparently pedophiles can only either grunt or...</td>\n",
       "      <td>Apparently pedophiles can only either grunt or...</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>2079</td>\n",
       "      <td>2019-07-14T23:48:01.000Z</td>\n",
       "      <td>2019-07-14T23:48:01.000Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              CommID authorDisplayName  \\\n",
       "0                         UgyMAptldbaotq25pgx4AaABAg        tomatoanus   \n",
       "1  UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93xcOkRbfQL  Nathaniel Huerta   \n",
       "2  UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93sUhWOu88m    Abe Abi Haydar   \n",
       "3  UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93eZplH5ug7          Yummyboi   \n",
       "4  UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93_dNCV8tK1          Jan Wick   \n",
       "5  UgyMAptldbaotq25pgx4AaABAg.8kLILEv0s_h93JojN1C7qc            qwerty   \n",
       "6                         Ugwga9AWIlVnajO3RR94AaABAg       Wendy McCoy   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/a/AGF-l7-dWGEdE94iYyfghH...   \n",
       "1  https://yt3.ggpht.com/a/AGF-l78sghDmpMleaviF44...   \n",
       "2  https://yt3.ggpht.com/a/AGF-l7-p_uJ-NlFNC1Sbjm...   \n",
       "3  https://yt3.ggpht.com/a/AGF-l7_yW6h1RXGK0MIe6F...   \n",
       "4  https://yt3.ggpht.com/a/AGF-l7--SwayUtPbOOD1Q0...   \n",
       "5  https://yt3.ggpht.com/a/AGF-l79llzvO-jCC7m5WLO...   \n",
       "6  https://yt3.ggpht.com/a/AGF-l7_lxlfY6eiEHLrWNg...   \n",
       "\n",
       "                                    authorChannelUrl  \\\n",
       "0  http://www.youtube.com/channel/UCm6GSA5OROHcIB...   \n",
       "1  http://www.youtube.com/channel/UC3hksSESHg3dDJ...   \n",
       "2  http://www.youtube.com/channel/UCNib--KiuE0TaG...   \n",
       "3  http://www.youtube.com/channel/UC_nuyvsPqhiHuF...   \n",
       "4  http://www.youtube.com/channel/UCZgWKtUWiHHK5f...   \n",
       "5  http://www.youtube.com/channel/UCC0Uq-HTbCA7uU...   \n",
       "6  http://www.youtube.com/channel/UCsPwH-Fb4ffPK2...   \n",
       "\n",
       "                         authorChannelId      videoId  \\\n",
       "0  {'value': 'UCm6GSA5OROHcIBNkXkH53zQ'}  rSLWFA_glbk   \n",
       "1  {'value': 'UC3hksSESHg3dDJmiK6kPaHQ'}  rSLWFA_glbk   \n",
       "2  {'value': 'UCNib--KiuE0TaGqIpUW9_sA'}  rSLWFA_glbk   \n",
       "3  {'value': 'UC_nuyvsPqhiHuF2X2XlELGg'}  rSLWFA_glbk   \n",
       "4  {'value': 'UCZgWKtUWiHHK5fz-AK4lK6w'}  rSLWFA_glbk   \n",
       "5  {'value': 'UCC0Uq-HTbCA7uUnHsGV6V8A'}  rSLWFA_glbk   \n",
       "6  {'value': 'UCsPwH-Fb4ffPK2Upamn65wQ'}  rSLWFA_glbk   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  he was just eatin some eggs outside a girls wi...   \n",
       "1                                He was stalking her   \n",
       "2                    @Pramey Sontakke stfu like stap   \n",
       "3                              literally everything不   \n",
       "4                                          mmmmm egg   \n",
       "5                            ITS ON A THURSDAY NIGHT   \n",
       "6  Apparently pedophiles can only either grunt or...   \n",
       "\n",
       "                                        textOriginal  canRate viewerRating  \\\n",
       "0  he was just eatin some eggs outside a girls wi...     True         none   \n",
       "1                                He was stalking her     True         none   \n",
       "2                    @Pramey Sontakke stfu like stap     True         none   \n",
       "3                              literally everything不     True         none   \n",
       "4                                          mmmmm egg     True         none   \n",
       "5                            ITS ON A THURSDAY NIGHT     True         none   \n",
       "6  Apparently pedophiles can only either grunt or...     True         none   \n",
       "\n",
       "   likeCount               publishedAt                 updatedAt  \\\n",
       "0      12447  2018-08-24T18:28:29.000Z  2018-08-24T18:28:29.000Z   \n",
       "1          0  2020-01-19T03:30:44.000Z  2020-01-19T03:30:44.000Z   \n",
       "2          0  2020-01-17T03:38:35.000Z  2020-01-17T03:38:35.000Z   \n",
       "3          0  2020-01-11T17:54:02.000Z  2020-01-11T17:54:02.000Z   \n",
       "4          0  2020-01-09T19:57:29.000Z  2020-01-09T19:57:29.000Z   \n",
       "5          0  2020-01-03T07:09:41.000Z  2020-01-03T07:09:41.000Z   \n",
       "6       2079  2019-07-14T23:48:01.000Z  2019-07-14T23:48:01.000Z   \n",
       "\n",
       "                     parentId  \n",
       "0                           0  \n",
       "1  UgyMAptldbaotq25pgx4AaABAg  \n",
       "2  UgyMAptldbaotq25pgx4AaABAg  \n",
       "3  UgyMAptldbaotq25pgx4AaABAg  \n",
       "4  UgyMAptldbaotq25pgx4AaABAg  \n",
       "5  UgyMAptldbaotq25pgx4AaABAg  \n",
       "6                           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drew1_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drew1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.yt_cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially ran:\n",
    "# drew2_df = cm.create_comments_df('iI48g7YTZFA')\n",
    "drew2_df = pd.read_csv(\"data/raw/drew2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drew2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_val(st):\n",
    "    return st['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c75c988bc981>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdrew2_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'authorChannelId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\insight\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3826\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m         \"\"\"\n\u001b[1;32m-> 3828\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3829\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\insight\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-24e0f3cd8219>\u001b[0m in \u001b[0;36minner_val\u001b[1;34m(st)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minner_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "drew2_df['authorChannelId'].map(inner_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having problems with the conversion of the commenters unique id value to a string only (i.e. not a dict) format. \n",
    "\n",
    "I have decided to go down the route of a psychographic market segmentation based on YouTube comments.\n",
    "The idea being that taking the first 100 comments from the (say) 30 latest videos of a channel would be give a good overall picture of the audience that watches that channel (assuming a number of things about the comments of course - no trolls, no bots, no artificial inflation of numbers, etc.)\n",
    "\n",
    "To keep the assumption at least partially true, I have tried to stick to channels that are non-political and have a vlog / talk to the camera style of production. This way the topics in each video hopefully will be varied enough that audience responses will also be quite varied and a good idea of what the interests of the audience actually is. Of course there will still be certain topics that the channel may concentrate on, but that may be useful for matching a new video to.\n",
    "\n",
    "The main idea is to try to use the Five-factor personality traits to put channels on a sliding scale of personality types. Categorise channel audiences according to these personality traits and therefore make it easier for marketers to target the audience they are looking for when advertising their products.\n",
    "\n",
    "# 2020-01-20\n",
    "\n",
    "Anyway, what I will do now is test out whether there are some simple (off-the-shelf) metrics I can use to show the similarity between two channels. Firstly, I will just start with analysis of a handful of videos.\n",
    "The two Drew Gooden (commentary / comedy) videos I have already downloaded, plus another third one from a seemingly distant video - say Jenn Im (beauty, travel, lifestyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially needed to save:\n",
    "# drew2_df.to_csv(\"data/raw/drew2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intially run:\n",
    "# jen1 = cm.create_comments_df('ZcCp6ghTP78', order_option='relevance', max_pgs=2)\n",
    "# jen1.to_csv('data/raw/jen1.csv')\n",
    "jen1_df = pd.read_csv('data/raw/jen1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drew1_wholetext = ''\n",
    "for text in drew1_df[drew1_df['parentId'] == '0']['textDisplay']:\n",
    "    drew1_wholetext = drew1_wholetext + '. ' + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drew1_wholetext[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drew1_wholetext[0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Test to see if the comment thread from the first half of the text matches the second.\n",
    "\n",
    "Need to tokenize and then run a similarity measure on it. I have mostly used the following guide to help me out here:\n",
    "https://kanoki.org/2018/12/27/text-matching-cosine-similarity/\n",
    "https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n",
    "\n",
    "The idea here being that we can use the tools from sklearn to do a number of tokenization / vectorization and even run cosine similarity on the text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.regexp import regexp_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n",
    "\n",
    "# Remove non-words:\n",
    "def clean_text(text):\n",
    "    # replace new line and return with space\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\",\" \")\n",
    "    # replace punctuation marks with space\n",
    "    punc_list = '!\"#$%&()*+,-./:;<=>?@[\\]{}|^_~' + '0123456789'\n",
    "    t=str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.translate(t)\n",
    "    # replace single quote with empty character\n",
    "    t = str.maketrans(dict.fromkeys(\"'`\",\"\"))\n",
    "    text = text.translate(t)\n",
    "    return text\n",
    "\n",
    "# Create a list of words:\n",
    "def renltk_tokenize(text):\n",
    "    text= clean_text(text)\n",
    "    # search for any white space and tokenize as word using regex '\\s+' \n",
    "    words = regexp_tokenize(text, pattern = '\\s+', gaps=True)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create whole text of the comments from the YouTube videos:\n",
    "\n",
    "drew1_wholetext = ''\n",
    "for text in drew1_df[drew1_df['parentId'] == '0']['textDisplay']:\n",
    "    drew1_wholetext = drew1_wholetext + '. ' + text\n",
    "    \n",
    "drew2_wholetext = ''\n",
    "for text in drew2_df[drew2_df['parentId'] == '0']['textDisplay']:\n",
    "    drew2_wholetext = drew2_wholetext + ' ' + text    \n",
    "\n",
    "jen1_wholetext = ''\n",
    "for text in jen1_df[jen1_df['parentId'] == '0']['textDisplay']:\n",
    "    jen1_wholetext = jen1_wholetext + ' ' + text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the comments sections into two halves for each video in order to get a test sample\n",
    "# Should expect most similarity between the two halves of the same drew video, less so between two drew videos\n",
    "# and even less between the jen videos and the drew videos:\n",
    "\n",
    "half1 = int(len(drew1_wholetext)/2)\n",
    "half2 = int(len(drew2_wholetext)/2)\n",
    "\n",
    "jhalf1 = int(len(jen1_wholetext)/2)\n",
    "\n",
    "drew1_part1 = drew1_wholetext[0:half1]\n",
    "drew1_part2 = drew1_wholetext[half1:]\n",
    "drew2_part1 = drew2_wholetext[0:half2]\n",
    "drew2_part2 = drew2_wholetext[half2:]\n",
    "jen1_part1 = jen1_wholetext[0:jhalf1]\n",
    "jen1_part2 = jen1_wholetext[jhalf1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://kanoki.org/2018/12/27/text-matching-cosine-similarity/\n",
    "\n",
    "corpus = [drew1_part1, drew1_part2, drew2_part1, drew2_part2, jen1_part1, jen1_part2]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "trsfm=vectorizer.fit_transform(corpus)\n",
    "#pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=['Drew1-1','Drew1-2','Drew2-1','Drew2-2'])\n",
    "parts_name = ['Drew1-1','Drew1-2','Drew2-1','Drew2-2','Jen1-1','Jen1-2']\n",
    "\n",
    "pd.DataFrame(cosine_similarity(trsfm, trsfm),index=parts_name,columns=parts_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the similarity between the Drew videos is high, but that of the Jen parts with each other is quite low.\n",
    "\n",
    "The Drew videos have many more comments than the Jenn videos, so therefore it would be a good idea to check what would happen if we restrict ourselves to the case of top 100 / 200 comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one text string of the top 100 comments from the YouTube videos:\n",
    "\n",
    "drew1_wholetext = ''\n",
    "for text in drew1_df[drew1_df['parentId'] == '0']['textDisplay'].head(100):\n",
    "    drew1_wholetext = drew1_wholetext + '. ' + text\n",
    "    \n",
    "drew2_wholetext = ''\n",
    "for text in drew2_df[drew2_df['parentId'] == '0']['textDisplay'].head(100):\n",
    "    drew2_wholetext = drew2_wholetext + ' ' + text    \n",
    "\n",
    "jen1_wholetext = ''\n",
    "for text in jen1_df[jen1_df['parentId'] == '0']['textDisplay'].head(100):\n",
    "    jen1_wholetext = jen1_wholetext + ' ' + text \n",
    "    \n",
    "half1 = int(len(drew1_wholetext)/2)\n",
    "half2 = int(len(drew2_wholetext)/2)\n",
    "\n",
    "jhalf1 = int(len(jen1_wholetext)/2)\n",
    "\n",
    "drew1_part1 = drew1_wholetext[0:half1]\n",
    "drew1_part2 = drew1_wholetext[half1:]\n",
    "drew2_part1 = drew2_wholetext[0:half2]\n",
    "drew2_part2 = drew2_wholetext[half2:]\n",
    "jen1_part1 = jen1_wholetext[0:jhalf1]\n",
    "jen1_part2 = jen1_wholetext[jhalf1:] \n",
    "\n",
    "corpus = [drew1_part1, drew1_part2, drew2_part1, drew2_part2, jen1_part1, jen1_part2]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "trsfm=vectorizer.fit_transform(corpus)\n",
    "parts_name = ['Drew1-1','Drew1-2','Drew2-1','Drew2-2','Jen1-1','Jen1-2']\n",
    "\n",
    "pd.DataFrame(cosine_similarity(trsfm, trsfm),index=parts_name,columns=parts_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are a lot closer now. Perhaps we can try top 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one text string of the top 200 comments from the YouTube videos:\n",
    "\n",
    "drew1_wholetext = ''\n",
    "for text in drew1_df[drew1_df['parentId'] == '0']['textDisplay'].head(200):\n",
    "    drew1_wholetext = drew1_wholetext + '. ' + text\n",
    "    \n",
    "drew2_wholetext = ''\n",
    "for text in drew2_df[drew2_df['parentId'] == '0']['textDisplay'].head(200):\n",
    "    drew2_wholetext = drew2_wholetext + ' ' + text    \n",
    "\n",
    "jen1_wholetext = ''\n",
    "for text in jen1_df[jen1_df['parentId'] == '0']['textDisplay'].head(200):\n",
    "    jen1_wholetext = jen1_wholetext + ' ' + text \n",
    "    \n",
    "half1 = int(len(drew1_wholetext)/2)\n",
    "half2 = int(len(drew2_wholetext)/2)\n",
    "\n",
    "jhalf1 = int(len(jen1_wholetext)/2)\n",
    "\n",
    "drew1_part1 = drew1_wholetext[0:half1]\n",
    "drew1_part2 = drew1_wholetext[half1:]\n",
    "drew2_part1 = drew2_wholetext[0:half2]\n",
    "drew2_part2 = drew2_wholetext[half2:]\n",
    "jen1_part1 = jen1_wholetext[0:jhalf1]\n",
    "jen1_part2 = jen1_wholetext[jhalf1:] \n",
    "\n",
    "corpus = [drew1_part1, drew1_part2, drew2_part1, drew2_part2, jen1_part1, jen1_part2]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "trsfm=vectorizer.fit_transform(corpus)\n",
    "parts_name = ['Drew1-1','Drew1-2','Drew2-1','Drew2-2','Jen1-1','Jen1-2']\n",
    "\n",
    "pd.DataFrame(cosine_similarity(trsfm, trsfm),index=parts_name,columns=parts_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely better with 200 comments, however the similarity between Drew1-2 and Drew2-2 is very similar to that of Jen1-2 (0.632 vs 0.631). There may in fact be some similarity in those specific comments, however it may be difficult to check.\n",
    "\n",
    "Perhaps I can try the Count Vectorizer and see how it compares to the tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "trsfm=vectorizer.fit_transform(corpus)\n",
    "parts_name = ['Drew1-1','Drew1-2','Drew2-1','Drew2-2','Jen1-1','Jen1-2']\n",
    "\n",
    "pd.DataFrame(cosine_similarity(trsfm, trsfm),index=parts_name,columns=parts_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity still remains quite high. Therefore I will stick with tf-idf for now.\n",
    "\n",
    "I need to develop the code for automatic video download given a YouTube video.\n",
    "\n",
    "Ideas:\n",
    " - Get a list of YouTube channel ids (based on the list I have curated of vloggers)\n",
    " - Run the ids through a function to find the 30 latest videos from that creator (ids)\n",
    " - Run the existing code to create a database of comment threads\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
